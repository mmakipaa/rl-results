{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL test run: Tabular methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook visualizes the results of testing selected Reinforcement Learning (RL) methods. \n",
    "\n",
    "Here, we present a comparison of different _tabular learning methods_. These methods store the approximate action-value functions to a table data structure. We learn for 100.000 episodes, which we would hope to be enough for reaching a reasonble  approximation of the value value function.\n",
    "\n",
    "For _epsilon_, we use the same scaled inverse visit count strategy, with _n0_ of 50. However, for Q-learning, a off-policy method, we apply a fully random behavior policy. For _alpha_, an exponential decay schedule is used, starting from 0.1 and reaching 0.05 at 90.000 learning iterations.\n",
    "\n",
    "Skip the intro and check the details of [the methods](#Evaluated-RL-methods), or get straight to the point [here](#Evaluation-of-method-performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context recap\n",
    "\n",
    "The notebook structure is repeated for different test runs with different method configurations in notebooks stored in this folder. \n",
    "\n",
    "Implementation of the evaluated RL methods can be found in repository https://github.com/mmakipaa/rl.\n",
    "\n",
    "Results have been created by running a test run based on yaml configuration file `configs\\tabular_agents.yaml` for a given number of iterations. In this case:\n",
    "\n",
    "```sh\n",
    "python run.py --environment blackjack --iterations 100000 --configfile tabular_agents\n",
    "```\n",
    "\n",
    "the results have been saved to a report file `blackjack_tabular_agents_100000.pik`. \n",
    "\n",
    "In this notebook, this report file is loaded, reported results are pre-processed and several visualizations are created in hope to unravel the hidden behavior of the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the testrun are stored as [pickle](https://docs.python.org/3/library/pickle.html)-serialized [Pandas](https://pandas.pydata.org/) `DataFrames`, so we import pickle, pandas and numpy. [Matplotlib](https://matplotlib.org/stable/index.html) and [Seaborn](https://seaborn.pydata.org/) are used for visualization. \n",
    "\n",
    "In addition, preprocessing and plotting utility functions are imported from `./utils` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import utils.plotting as plotting\n",
    "from utils.process_report import process_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure settings\n",
    "\n",
    "The aim is to create relatively large png images in 4:3 aspect ratio. Alternative would be to create inline images in svg format.\n",
    "\n",
    "To adjust image size for side-by-side gridplots, see `get_sidebyside_imgsize` in `utils\\plotting.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_SIZE=(12, 9)\n",
    "FIG_DPI=300\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "# alternatively - %config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report data\n",
    "\n",
    "Test run results are loaded from `report_filename` and reference result from `reference_filename` defined in the cell below. The naming convention for pickled report filenames is: `<environment>_<configfile>_<iterations>.pik`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = Path(\"../testruns/\")\n",
    "\n",
    "report_filename = ( \"blackjack_tabular_agents_100000.pik\" )\n",
    "reference_filename = 'blackjack_ref_agent_100000000.pik'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_file = FOLDER / report_filename\n",
    "\n",
    "with open(report_file,'rb') as f:\n",
    "    report_dict = pickle.load(f)\n",
    "    agents = report_dict['agents']\n",
    "    df_report = report_dict['report']\n",
    "    df_ev_revards = report_dict['ev_rewards']\n",
    "\n",
    "reference_file = FOLDER / reference_filename\n",
    "    \n",
    "with open(reference_file,'rb') as f:\n",
    "    ref_dict = pickle.load(f)\n",
    "    ref_agents = ref_dict['agents']\n",
    "    df_reference = ref_dict['report']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluated RL methods\n",
    "\n",
    "The report file loaded above contains results for the following RL methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agents = pd.DataFrame(agents)\n",
    "df_agents[['name','method']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method descriptions are as follows (from [rl/README.md](https://github.com/mmakipaa/rl/blob/main/README.md)):\n",
    "\n",
    "| Method key| Description |\n",
    "| --- | --- |\n",
    "| MonteCarloOn | On-policy Monte Carlo, full episodes and tabular value representation |\n",
    "| MonteCarloOff | Off-policy Monte Carlo, full episodes and tabular value representation |\n",
    "| Sarsa | Sarsa TD(0) using tabular value representation |\n",
    "| SarsaExpected | Expected Sarsa TD(0) using tabular value representation |\n",
    "| Qlearning | Q-learning TD(0) using tabular value representation |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning parameters `epsilon` and `alpha` and corresponding learning schedules are detailed in the tables below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_agents.filter(regex='name|epsilon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df_agents.filter(regex='name|alpha'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting points\n",
    "\n",
    "Action-values and visit counts are reported for each state-action pair at log-intervals as indicated in column `iterations`. Reports have been logged at the following points during learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = list(df_report['iterations'].unique())\n",
    "MAX_ITERATIONS = iterations[-1]\n",
    "display(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference result\n",
    "\n",
    "As we do not have access to the true value function of Blackjack, we approximate the value function with a reference result, that we believe to be close to the true value function.\n",
    "\n",
    "The reference result has been obtained by running off-policy MonteCarlo method with random behavior policy for 100 000 000 episodes, with configuration as shown below.\n",
    "\n",
    "When we in the following compare the value function learned by each method to this reference result, it is good to understand that this comparison is not without caveats: if the reference result is wrong, so will be the comparisons presented below, such as MSE error or wrong policy decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ref_agents)\n",
    "display(df_reference['iterations'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing report data\n",
    "\n",
    "The report DataFrame,`df_report`, records the estimated action-value function for each state-action pair. State is represented as columns `(dealer, player, soft)`, where `dealer` corresponds to sum of dealer's cards, `player` to sum of player's cards, and `soft` indicates whether player has a soft (usable) ace. Column `action` indicates the action; True corresponds to `hit` and False corresponds to `stand`.\n",
    "\n",
    "The report contains the state-action pairs for each agent included in the test run at each reporting iteration. `df_reference` contains a similar report for the reference method.\n",
    "\n",
    "In addition to state-action values, state-action visit counts are recorded for tabular methods. Visit counts are not available for approximate semi-gradient or batch methods.\n",
    "\n",
    "Next, we run the `process_report` utility to pre-process the report files and to create the following DataFrames:\n",
    "\n",
    "* `df_ref` - augments the reference results to indicate whether actions is the optimal action in that state\n",
    "\n",
    "* `df_ref_change_points` - indicates the points for different combinations of dealers cards and soft ace, where optimal action changes from `hit` to `stand`.\n",
    "\n",
    "* `df_sa` - augments the report DataFrame to indicate whether an action is the optimal one in that state. We also add the reference values and optimal action labels, and calculate the error between values of the tested method and our reference.\n",
    "\n",
    "* `df_states` - rolls the state-action pairs of `df_sa`into states, calculating visit counts for each state, as well as state min and max values and optimal actions. Additionally labels the states for plotting policies on state grid later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_ref, df_ref_change_points, df_sa, df_states = process_report(df_report, df_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of method performance\n",
    "\n",
    "The allow comparison of the methods, we run simple performance metrics for each tested method in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation rewards\n",
    "\n",
    "Ten thousand additional evaluation episodes were run using the value function and corresponding greedy policy reached at the end of learning. The combined rewards received during evaluation are shown below.\n",
    "\n",
    "Note that also training time rewards would be available as `report_dict['tr_rewards']`. We do not, however, consider training time rewards as a metric at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ev_revards['per_episode'] = df_ev_revards['reward'] / df_ev_revards['episodes']\n",
    "df_ev_revards['rank'] = df_ev_revards['per_episode'].rank(ascending=False)\n",
    "df_ev_revards = df_ev_revards.sort_values(by='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_ev_revards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-squared error \n",
    "\n",
    "We calculate the mean-squared error of the action-value function against the reference and display the sorted order of methods below.\n",
    "\n",
    "As we are comparing to the reference case, mean squared error of zero would mean that the evaluated method would have learned exactly the same action-value function as the reference method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = df_sa.groupby(['agent', 'iterations'])\n",
    "df_mse = dg['sq_error'].mean().reset_index()\n",
    "\n",
    "df_mse_last = df_mse.loc[df_states.groupby(['agent'])['iterations'].idxmax()]\n",
    "df_mse_last['rank'] = df_mse_last['sq_error'].rank(ascending=True)\n",
    "df_mse_last = df_mse_last.sort_values(by='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_mse_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrong action count\n",
    "\n",
    "We calculate the number of states where the agent would make a wrong decision, i.e. the policy learned would not select the same optimal action as the reference method at the end of learning. Thus, zero wrong actions selected would mean that the method has learned the same greedy policy as the refence method.\n",
    "\n",
    "There are 280 states and 560 state-action pairs. Initially, when action-values (or weights for approximate methods) have been initialized to same initial value, typically zero, a unique best action is not available. In this case, the policy decisions in 280 states would be made randomly, resulting in hopefully getting half of the decisions right. This explains the intial level from which we hope to improve during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wrong_action_count(x):\n",
    "    # Assume we would get half of states with ties right\n",
    "    compensation = np.floor(x['s_best_action'].isna().sum() / 2)\n",
    "    return sum(x['s_best_action'] != x['ref_best_action']) - compensation\n",
    "\n",
    "df_wrong = df_states.groupby(['agent', 'iterations']).apply(get_wrong_action_count).reset_index(name ='wrong_count')\n",
    "\n",
    "df_wrong_last = df_wrong.loc[df_states.groupby(['agent'])['iterations'].idxmax()]\n",
    "df_wrong_last['rank'] = df_wrong_last['wrong_count'].rank(ascending=True)\n",
    "df_wrong_last = df_wrong_last.sort_values('rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_wrong_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple performance ranking\n",
    "We collect the above performance metrics by calculating the combined rank, column `sumrank` in the DataFrame below, to provide a simple performance ranking for the evaluated methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agent_ranking = pd.concat([df_wrong_last, df_mse_last, df_ev_revards]).groupby('agent') \\\n",
    "                            .sum().drop(['iterations','episodes'], axis=1) \\\n",
    "                            .rename(columns={\"rank\": \"sumrank\"}) \\\n",
    "                            .sort_values(by=['sumrank','per_episode', 'wrong_count','sq_error']) \\\n",
    "                            .reset_index()\n",
    "display(df_agent_ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing convergence over learning iterations\n",
    "\n",
    "In the following we show the evolution of mean squared error and number of states where a wrong decision is made over learning iterations.\n",
    "\n",
    "Note that in the following plots we use logarithmic scale on the x-axis as the reports were collected at log-itervals during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALE = 'LINEAR'\n",
    "SCALE = 'LOG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=FIG_SIZE, dpi=FIG_DPI)\n",
    "ax = sns.lineplot(data=df_mse, x=\"iterations\", y=\"sq_error\", hue='agent', linewidth=1, marker='o', markersize=4)\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "if SCALE == 'LOG':\n",
    "    _min_xlim = 0.9 * df_report['iterations'].loc[df_report['iterations'] > 0].min()\n",
    "else:\n",
    "    _min_xlim = 0\n",
    "_max_xlim = MAX_ITERATIONS * 1.05\n",
    "_min_ylim = 0\n",
    "_max_ylim = df_mse['sq_error'].max() * 1.05\n",
    "    \n",
    "plotting.format_ax(ax, _min_xlim, _max_xlim, _min_ylim, _max_ylim, SCALE)\n",
    "\n",
    "ax.set_ylabel(\"MSE\")\n",
    "\n",
    "_ = ax.set_title(\"Mean Sqared Error (compared to reference)\", fontdict={'fontsize': 12}, y=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=FIG_SIZE, dpi=FIG_DPI)\n",
    "ax = sns.lineplot(data=df_wrong, x=\"iterations\", y=\"wrong_count\", hue='agent',linewidth=1, marker='o', markersize=4)\n",
    "\n",
    "if SCALE == 'LOG':\n",
    "    _min_xlim = 0.9 * df_report['iterations'].loc[df_report['iterations'] > 0].min()\n",
    "else:\n",
    "    _min_xlim = 0\n",
    "_max_xlim = MAX_ITERATIONS * 1.05\n",
    "_min_ylim = 0\n",
    "_max_ylim = df_wrong['wrong_count'].max() * 1.05\n",
    "    \n",
    "plotting.format_ax(ax, _min_xlim, _max_xlim, _min_ylim, _max_ylim, SCALE)\n",
    "\n",
    "ax.set_ylabel(\"Number of states with wrong actions selected\")\n",
    "\n",
    "_ = ax.set_title(\"Wrong actions (compared to reference)\", fontdict={'fontsize': 12}, y=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the action-value function\n",
    "\n",
    "In the following, we visualize the action-value functions for each of the methods at the end of learning.\n",
    "\n",
    "For clarity, we divide the state-action pairs into four groups, each corresponding to state-actions in pair `(soft, action)`. For instance, `soft=True` and `action=STAND` would mean that the player has a soft (usable) ace valued at 11 and chooses to stand, i.e. not to take any more cards.\n",
    "\n",
    "For each group, x-axis shows the different sums of players cards (from 4 to 21) and y-axis gives the action-value. Each plotted line shows a different value for dealers cards (from 2 to 11, or ten different lines with different hues from the palette shown below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(plotting.get_colormap(\"dealer\")[0]([i / 10 for i in range(0,10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reference value-function is shown in dotted lines and gray hues.\n",
    "\n",
    "The plots are ordered according to our simple performance ranking calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#fig = plt.figure(figsize=FIG_SIZE, dpi=FIG_DPI)\n",
    "\n",
    "ratio = 3 / 4\n",
    "width = 12\n",
    "height = ratio * 2 / 2 * width\n",
    "\n",
    "max_iterations = df_sa.loc[df_sa.groupby(['agent'])['iterations'].idxmax(),['agent','iterations']]\n",
    "\n",
    "for current_agent in df_agent_ranking['agent'].tolist():\n",
    "    fig = plt.figure(figsize=(width, height), dpi=FIG_DPI)\n",
    "    current_set = df_sa.loc[((df_sa['agent'] == current_agent) &\n",
    "                      (df_sa['iterations'] == max_iterations.loc[max_iterations['agent'] == current_agent,'iterations'].item()))]\n",
    "    \n",
    "    for i, pair in enumerate(itertools.product((False, True), repeat=2)):\n",
    "\n",
    "        _action = pair[0]\n",
    "        _soft = pair[1]\n",
    "\n",
    "        _df_vis = current_set.loc[(current_set['action'] == _action) &(current_set['soft'] == _soft)]\n",
    "        \n",
    "        title = f\"{current_agent}: Action: {('Hit' if _action else 'Stand')}, Soft: {_soft}\" \n",
    "        plotting.plot_value_subplot(fig, i+1, _df_vis, title)\n",
    "    \n",
    "    title_str = current_agent + \": \"\n",
    "    title_str += f\"Wrong: {df_agent_ranking.loc[df_agent_ranking['agent'] == current_agent,'wrong_count'].item():.0f}\"\n",
    "    title_str += f\", MSE: {df_agent_ranking.loc[df_agent_ranking['agent'] == current_agent,'sq_error'].item():.04f}\"\n",
    "    title_str += f\", Reward: {df_agent_ranking.loc[df_agent_ranking['agent'] == current_agent,'reward'].item():.0f}\"\n",
    "\n",
    "    fig.suptitle(title_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option to conserve energy...\n",
    "\n",
    "To limit the number of subplots created in the side-by-side visualizations that follow, we can filter the iterations at which we plot intermediate values.\n",
    "\n",
    "For example, the following discards every other iteration included in the report file, keeping the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_iterations = iterations[1::2] + [iterations[-1]]\n",
    "display(sbs_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image size for following visualizations set as (inches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbs_fig_w, sbs_fig_h = plotting.get_sidebyside_imgsize(len(agents), len(sbs_iterations))\n",
    "display((sbs_fig_w, sbs_fig_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action-value function over iterations\n",
    "\n",
    "To dig deeper, we visualize the development of action-value function over learning iterations. \n",
    "\n",
    "In the subplot grid, each column shows results for a RL method. Each row displays the value function at an iteration during learning, with last row showing the final result. Here one iteration of learning corresponds to an episode: Starting from 100 iterations, the agent has completed 100 episodes of play at that point.\n",
    "\n",
    "We divide the plots in two, first plotting the states `(dealer, player, soft)` with no soft ace, i.e. `soft=False` and then repeating for states with soft ace.\n",
    "\n",
    "Within a subplot, each square on the grid corresponds to a state and shows the estimate of _value of the state following greedy policy_. \n",
    "\n",
    "As with previous action-value plots, sum for dealer's cards on vertical grid axis varies from 2 to 11, and sum of player's cards on the horizontal axis from 4 to 21.\n",
    "\n",
    "Dark green hues corresponds to action-values close to 1 (i.e. likely to win if choosing the optimal action) and dark purple hues to action-values close to -1 (likely to lose), utilizing this palette. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(plotting.get_colormap(\"values\")[0]([i / 50 for i in range(0,51)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No soft ace\n",
    "\n",
    "First, we plot the values for `soft = False`, the player has no usable soft ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soft = False\n",
    "\n",
    "fig = plt.figure(figsize=(sbs_fig_w, sbs_fig_h ), dpi=FIG_DPI)\n",
    "\n",
    "plotting.plot_heatmaps_sidebyside(fig, df_states, 's_max_value', agents=agents, iterations=sbs_iterations, \n",
    "                         soft=soft, content_type='values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With soft ace\n",
    "\n",
    "We repeat the plot of optimal value functions for the states with soft ace. Note that the minimum sum for player is now 12: a usable ace counted as 11 and a non-usable ace counted as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soft = True\n",
    "\n",
    "fig = plt.figure(figsize=(sbs_fig_w, sbs_fig_h ), dpi=FIG_DPI)\n",
    "\n",
    "plotting.plot_heatmaps_sidebyside(fig, df_states, 's_max_value', agents=agents, iterations=sbs_iterations, \n",
    "                         soft=soft, content_type='values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy over iterations\n",
    "\n",
    "Next, we visualize how the greedy policy changes over iterations. Orange squares correspond to states where the optimal action is to `hit`, blue to states where optimal action is to `stand`. Again, for each subplot, the dealers hand varies along horizontal and players hand along vertical axis.\n",
    "\n",
    "For tabular methods, we can additionally illustrate states where only one of the actions has been visited in light orange and blue hues. States with tied values for actions are shown in gray and non-visited states in light gray. The colormap is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(plotting.get_colormap(\"actions\")[0]([i / 50 for i in range(0,51)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No soft ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soft = False\n",
    "\n",
    "fig = plt.figure(figsize=(sbs_fig_w, sbs_fig_h ), dpi=FIG_DPI)\n",
    "\n",
    "plotting.plot_heatmaps_sidebyside(fig, df_states, 's_label', agents=agents, iterations=sbs_iterations, soft=soft,\\\n",
    "                         content_type='actions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With soft ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soft = True\n",
    "\n",
    "fig = plt.figure(figsize=(sbs_fig_w, sbs_fig_h ), dpi=FIG_DPI)\n",
    "\n",
    "plotting.plot_heatmaps_sidebyside(fig, df_states, 's_label', agents=agents, iterations=sbs_iterations, soft=soft,\\\n",
    "                         content_type='actions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference in value between actions\n",
    "\n",
    "Finally, we illustrate the difference in value between actions in a state and how the difference changes over iterations. \n",
    "\n",
    "A light hue indicates little difference in value between the actions. This can be the case in the early phases of learning, when the values of actions have not yet converged, or simply because the values are close - there is not much difference in expected returns regardless of what action is chosen.\n",
    "\n",
    "A dark blue hue represents a state where the difference in the value of actions approaches maximum value of 2: Selecting one action would have the value of 1 and the other action the value of -1. This is the case for example for states where player's sum is 21 - taking an additional card is a sure path to losing, resulting in expected reward of -1. Standing at 21 on the other hand has a small probability of a tie with reward of 0, but winning is quite likely, giving expected reward of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.palplot(plotting.get_colormap(\"value_diffs\")[0]([i / 50 for i in range(0, 51)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No soft ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soft = False\n",
    "\n",
    "fig = plt.figure(figsize=(sbs_fig_w, sbs_fig_h ), dpi=FIG_DPI)\n",
    "\n",
    "plotting.plot_heatmaps_sidebyside(fig, df_states, 's_diff', agents=agents, iterations=sbs_iterations,\n",
    "                         soft=soft, content_type='value_diffs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With soft ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soft = True\n",
    "\n",
    "fig = plt.figure(figsize=(sbs_fig_w, sbs_fig_h ), dpi=FIG_DPI)\n",
    "\n",
    "plotting.plot_heatmaps_sidebyside(fig, df_states, 's_diff', agents=agents, iterations=sbs_iterations,\n",
    "                         soft=soft, content_type='value_diffs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
